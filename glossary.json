{
    "recognition": "\nThis refers to the process of attaching **_semantic_ category labels** to objects, scenes, events, and activities in images. [\\[1\\]][1]\n\n",
    "reconstruction": "\nTraditionally, this involves the **recovery of three-dimensional geometry** from images.\\\nMore broadly, it can be interpreted as **\"inverse graphics\"**: estimating shape, spatial layout, reflectance, and illumination. [\\[1\\]][1]\n\n",
    "reorganization": "\nThis refers to the **grouping/segmentation** of visual elements.<br>It is the computer vision analog of _perceptual organization_ from Gestalt psychology. [\\[1\\]][1]\n\n[1]: https://www.sciencedirect.com/science/article/pii/S0167865516000313\n\n",
    "signal": "\nA signal is a **function** of some variable(s), often time or space.\\\nIts inputs and outputs typically have physical meaning &mdash; for instance, an image can be understood as a signal: brightness as a function of position.\n\n",
    "quantization": "\nThis is the process of mapping values from a **larger, continuous, and/or infinite set** to values in a **smaller, discrete, and/or finite set**.<br>It is the basis of discrete signal and image processing.\n\n",
    "resolution": "\nFor our purposes ([slide 18](https://drive.google.com/file/d/15rw06o8WOnqjfQMjPjdSkrHJ1Qhn-4R9/view)):\\\n**Spatial resolution** refers to the linear spacing of a measurement, which, in the context of images, corresponds to the physical separation represented by a pixel &mdash; this could be an angle or a distance.\\\n**Geometric resolution** is closer in meaning to [angular resolution](https://en.wikipedia.org/wiki/Angular_resolution), which relates to [resolving power](https://en.wikipedia.org/wiki/Angular_resolution#Definition_of_terms), the Rayleigh criterion, and how \"blurry\" images are.\n\n",
    "filter": "\nAs generally as possible, a filter is a **function of the local neighborhood** which operates on a signal.\\\nThis function can be continuous or discrete, linear or non-linear, ... etc, and the \"local neighborhood\" can be any size: from a single point to an infinite extent.\n\n",
    "image-filtering": "\nImage filtering is the computation of a **function of the local neighborhood** of an image **at each position**.\\\nIt may be used to _enhance_, _extract information_ from, or _detect patterns_ in images.\\\n\\\n**Image filtering** can be implemented by **convolving** an **image** and a **filter**.\\\n\n",
    "convolution": "\nAs generally as possible, convolution is an **operation** which takes two functions and produces a third function, one whose value at some point X is the **integral** of the **product** of the first function and the second function **flipped in the x direction then offset by X**. Whew!\\\n\\\nIn the context of digital image processing, \"integrals\" are just sums over matrix elements, \"products\" refer to Hadamard products, and \"flipping\" is just rotating a 2D matrix by 180\u00b0. `Todo: add link to animation`\\\n\\\n**Important: _convolution is both commutative and associative._**\n\n",
    "correlation": "\nCorrelation is the same as convolution, but **without the flipping**.\\\nObserve that if you are using a 180\u00b0-rotationally-symmetric kernel, then convolution and correlation are identical.\\\n\\\n**Important: _correlation is neither commutative nor associative._**\n\n",
    "kernel": "\nIn the context of digital image processing, a kernel is a **2D matrix** which acts as a filter when **convolved** with another 2D matrix, typically an image.\n\n",
    "separability": "\nWhen used to describe a kernel, separability refers to the kernel's ability to be **factored out** as the **product of two 1D kernels** (one row and one column vector).\\\n\\\nGiven a separable kernel `K` which factors out into `R` and `C`, and an image `I`:\\\n`K * I == R * (C * I) == C * (R * I)`, where `*` represents the convolution operator.\n\n",
    "linearity-and-shift-invariance": "\nThese are properties of operations.\\\n\\\nSuppose you have some operation `T` such that `y(t) = T( x(t) )`.\\\nIf `T` is linear: `T( a * x1(t) + b * x2(t) ) = a * y1(t) + b * y2(t)`\\\nIf `T` is shift-invariant: `y(t - s) = T( x(t - s) )`\\\n\\\nAny operation which is **both linear and shift-invariant** can be represented as a `convolution`.\\\nConvolution itself is also linear and shift-invariant.\n\n",
    "aliasing": "\nAliasing refers to when a signal becomes **indistinguishable** from a different signal, due to **sampling**.\\\n\\\nAn example is when car wheels appear to spin the wrong way in videos &mdash; the orientation of the wheel with time is the signal, and the video's frames are the samples. If your sampling rate is too low, a fast clockwise rotation can look exactly like a counter-clockwise one.\n\n",
    "nyquist-shannon-sampling-theorem": "\nThe Nyquist-Shannon Sampling Theorem provides a rule for sampling which prevents aliasing:\\\n\\\n_When sampling a signal **at discrete intervals**, the sampling frequency must be `\u2265 2 * f_max`, where `f_max` is the absolute maximum frequency of the input signal._\\\n\\\nIf this rule is followed, it is possible reconstruct the original signal _perfectly_ from its samples &mdash; lossless compression.\n\n",
    "preventing-aliasing": "\nFollowing the Nyquist-Shannon sampling theorem, you can either:\\\na. Increase the sampling rate, or\\\nb. Decrease the maximum frequency of the input signal. This can be done via Gaussian filtering.\n\n",
    "hybrid-images": "\nThese images are formed by combining the high-frequency components of one image with the low-frequency components of another image.\\\nThe result is a third \"hybrid\" image that looks like either the first or the second image, depending on the scale at which it is viewed.\n\n",
    "template-matching": "\nThis refers to image filtering when viewed as \"comparing an **image of what you want to find** (as the filter) against another image\".\\\n\\\nThis involves zero-centering your selected filter by subtracting the mean of its pixels, then correlating the filter with the image (or equivalently, convolving the flipped filter with the image).\n\n",
    "fourier-theorem": "\n_Any univariate function can be rewritten as a weighted sum of sines and cosines of different frequencies._\n\n",
    "fourier-transform-decomposition-series": "\nThe Fourier **transform** of a function is the representation of that function as a weighted sum of **Fourier basis functions**. The process of breaking a function into its Fourier basis functions is known as Fourier **decomposition**.\\\n\\\nThe Fourier **series** is similar to the Fourier transform, but it is used exclusively for _periodic_ functions. So, unless you're taking some strange photos, you'll probably want the Fourier transform instead.\n\n",
    "fourier-basis-functions": "\nThese are simply sines and cosines of different (1) **amplitudes** (weights) and (2) **frequencies**.\\\n\\\nIn 2D Fourier decomposition (of 2D images, say), we use 2D sinusoids: amplitude (and phase) are scalar values just as with 1D sinusoids, but frequency is now a 2D vector, since you need to account for rate of change in both directions.\n\n",
    "amplitude-phase-form": "\nThe amplitude-phase form aims to encode the Fourier transform of an image.\\\n\\\nRecall that the sum of a sine and a cosine function, each with some amplitude but the same frequency, is simply a third sinusoid with some **phase offset**. Thus, we can represent every term of a Fourier decomposition with three values: (1) amplitude, (2) frequency, and (3) phase.\\\n\\\nThe amplitude-phase form encodes this information in two signals: (A) amplitude as a function of frequency, and (B) phase as a function of frequency.\\\nBecause we're using 2D sinusoids, frequency is 2D: therefore, signals (A) and (B) are typically represented as images, where the position of each pixel is the frequency, and the intensity of that pixel is the corresponding amplitude/phase of the term with that frequency.\n\n",
    "spatial-domain-vs-fourierfrequency-domain": "\nRecall that an image can be thought of as brightness as a function of 2D **position**. Since the input is a point in space, we can say that this image is in the spatial domain.\\\nAn amplitude-phase form image, on the other hand, is amplitude/phase as a function of 2D **frequency**, and is thus in the Fourier/frequency domain.\n\n",
    "the-convolution-theorem": "\nConvolution in the spatial domain is equivalent to (element-wise) multiplication in the frequency domain.\\\nConsequently, the **Fourier transform** of the convolution of two functions is the product of their Fourier transforms.\n\n",
    "image-filtering-in-the-frequency-domain": "\nRecall that image filtering is implemented by the convolution of an **image** and a **filter**. Now that we understand **the convolution theorem**, we can view image filtering as the product of the Fourier transforms of the image and the filter.\n\n",
    "box--sinc-dual": "\nThe Fourier transform of a box function is a sinc function, and vice versa.\\\nThis is slightly troublesome: a box function in the frequency domain would be an ideal low-pass filter, but to implement it, you'd need a filter that looks like a sinc function in the spatial domain.\\\nUnfortunately, sinc functions are infinite in extent, and we do not have infinitely-wide filters.\n\n",
    "artifacts": "\nWhat if you tried to blur an image with a **box filter**?\\\n\\\nYou'd get artifacts. The Fourier transform of a box filter is a sinc, which has non-zero components in the high-frequency range. Consequently, your output image will retain any existing high-frequency components.\\\n`Todo: add link to example image`\n\n",
    "ringing-artifacts--gibbs-phenomenon": "\nWhat if you tried to blur an image with an **approximation of a sinc filter**?\\\n\\\nYou'd still get artifacts. Because the approximation is imperfect, the Fourier transform of this filter will be an imperfect box with **overshoots near the discontinuities** (see: [Gibbs phenomenon](https://en.wikipedia.org/wiki/Gibbs_phenomenon)). Consequently, your output image will exhibit \"ringing\" artifacts near edges.\\\n`Todo: add link to example image`\n\n",
    "edges": "\nAn edge is a place of rapid change in the image intensity function.\\\nThis change can be in overall brightness, or in colors &mdash; consider a sudden jump from #FF0000 (red) to ##00FF00 (blue).\\\n\\\nEdges are \"high-frequency content\" &mdash; that is, they correspond to the image's high frequency components.\n\n",
    "edge-detection-via-taking-the-derivative": "\nBecause of their association with _high rate of change_, edges correspond directly to **extrema in the first derivative of image signals**.\\\nHowever, because of the presence of noise, we can't simply take the derivative of an image &mdash; it must first be smoothed (e.g. with a Gaussian filter).\n\n",
    "1d-gaussian-filter": "\nA Gaussian filter is a filter whose shape in the spatial domain is a Gaussian function.\\\nInterestingly, its Fourier transform (i.e. its shape in the frequency domain) is simply another Gaussian (with inverted sigma). \\\n**It is thus useful as a low-pass filter, e.g. for _blurring/smoothing_**.\\\n\\\nEven though the Gaussian function is technically of infinite extent, in practice, it is effectively zero three standard deviations away from the mean, which is why we can approximate it fairly well with discrete Gaussian kernels.\n\n",
    "2d-gaussian-filter": "\nThe above holds true for the 2D Gaussian filter.\\\n2D Gaussian filters are **separable**.\n\n",
    "the-1st-derivative-of-a-gaussian": "\nRecall that in order to find edges, we need to (1) **blur** (convolve by a Gaussian), then (2) **differentiate** (convolve by a kernel which achieves differentiation).\\\nSince convolution is [**differentiable**](https://en.wikipedia.org/wiki/Convolution#Differentiation), we can combine these two steps, and instead convolve our image by the 1st derivative of a Gaussian.\n\n",
    "detection-and-localization": "\nThese are qualities of edge detectors.\\\n\\\n**Detection** refers to the detector's ability to find all real edges, ignoring noise and other artifacts.\\\n**Localization** refers to the detector's ability to return a single-point output close to the true edge.\n\n",
    "canny-edge-detector": "\nThe [**Canny edge detector**](https://ieeexplore.ieee.org/document/4767851) is \"probably the most widely-used edge detector in computer vision\".\\\nPlease refer to the slides for steps!\n\n",
    "non-maximum-suppression": "\nThis refers to the reduction of multi-pixel wide \"ridges\" to single-pixel wide lines, which is achieved by discarding pixels which are not the local maximum along the direction of most change (the gradient).\n\n",
    "hysteresis-thresholding": "\nRegular thresholding involves discarding areas below a certain fixed threshold value.\\\nThis can easily be improved by setting, say, two thresholds corresponding to \"weak\" and \"strong\" edges.\\\n**Hysteresis** thresholding further improves on this, by discarding \"weak\" edges which are not connected to \"strong\" edges. Refer to the [`scikit-image` documentation](https://scikit-image.org/docs/dev/auto_examples/filters/plot_hysteresis.html) for a visual example.\n\n",
    "features": "\nThese are parts of an image which are distinctive and likely useful for computing similarities between images.\\\nA feature typically consists of (1) a **key/interest point**, which has some position, and (2) a **feature descriptor**, which encodes some information about the point's immediate neighborhood / surrounding patch in the image. Either (1) or (2) might also include scale or orientation information, among other things.\n\n",
    "detection-description-and-matching": "\nThese are the three steps involved in identifying feature point correspondences between multiple images.\\\n\\\n**Detection** refers to the process of finding **key points**.\\\n**Description** refers to the process of extracting **feature descriptors** from the areas around each key point.\\\n**Matching** refers to the process of comparing the **features** of two or more images, and figuring out which features in each correspond to features in the others.\\\n\n",
    "distinctiveness-repeatability-and-compactnessefficiency": "\nThese are qualities of feature representations.\\\n\\\n**Distinctiveness** refers to the ability to uniquely identify a point, which may be challenging, regardless of representation, in images with repeated elements. This can be\\\n**Repeatability** refers to the ability to locate the same feature in multiple images despite _geometric_ and _photometric_ differences.\\\n**Compactness/Efficiency** refers to the ability of the representation to be as small (compact) as possible, for performance.\\\n\n",
    "geometric-transformations": "\nThese refer to transformations in translation, rotation, scale, and perspective, etc.\n\n",
    "photometric-transformations": "\nThese refer to transformations in reflectance and illumination, etc.\n\n",
    "corners": "\nTo get _distinctive_ and _repeatable_ features, we want to look for points which are stable in appearance, with respect to small variations in position.\\\nThus, we choose to look for **corners**: when looking at them through a small **window**, shifting away from them in _any direction_ will cause a large change in the window's overall intensity.\n\n",
    "corner-detection-by-auto-correlation": "\nIn this case, auto-correlation refers to our method of looking through a window, evaluating intensity, and comparing it with the intensity resulting from a shifted window. We are effectively correlating (a part of) the image with (another part of) itself, hence auto-correlation.\\\n\\\nWe would like to do this for every point in the image, and keep the points where the auto-correlation function looks like a strong **peak**. You might say that such points have a high \"corner-ness\" score.\\\n\\\nUnfortunately, this requires a lot of computation, far too costly for our purposes.\n\n",
    "harris-corner-detection": "\nThe Harris corner detector solves the above problem by way of approximations and linear-algebraic manipulations, ultimately reducing the cost of computing a \"cornerness\" score for each point (see the entry for the _Harris Cornerness Score_ below).\\\nPlease refer to the slides for steps!\n\n",
    "taylor-series-expansion": "\nThe Taylor series of a function is an infinite series of terms that are expressed in terms of the function's derivatives at a single point. Most functions are equal to the sum of their Taylor series near that point.\\\n\\\nTaylor series expansion refers to the process of finding these terms for a given function.\\\nIt was one of tools used to arrive at the Harris corner detector algorithm, in approximating the computation of the auto-correlation function.\n\n",
    "second-moment-matrix-m": "\nThis is a 2x2 matrix with the following elements:\\\n`\u23a1 \u03a3( I_x ^ 2 ), \u03a3( I_x * I_y ) \u23a4`\\\n`\u23a3 \u03a3( I_x * I_y ), \u03a3( I_y ^ 2 ) \u23a6`\\\nWhere `I_x` and `I_y` refer to the image derivatives, at some point, in the x and y directions respectively. These summations are over the window described earlier.\\\n\\\nThis matrix is square, [symmetric](https://en.wikipedia.org/wiki/Symmetric_matrix), and [diagonalizable](https://en.wikipedia.org/wiki/Diagonalizable_matrix).\\\nIt has two eigenvalues `\u03bb_1` and `\u03bb_2` &mdash; please refer to the slides for their visual interpretation!\n\n",
    "harris-cornerness-score": "\nThis is equal to `C`, where `C = \u03bb_1 * \u03bb_2 - \u03b1 * (\u03bb_1 + \u03bb_2) ^ 2` and `\u03b1` is some constant around 0.04 - 0.06.\\\n\\\nWith the second moment matrix, its determinant is equal to the product of its eigenvalues, and its trace is the sum of its eigenvalues, so this equation can be rewritten as: `C = det(M) - \u03b1 * trace(M) ^ 2`, thereby avoiding even having to calculate the eigenvalues.\n\n",
    "invariance-and-covariance": "\nLoosely, invariance = \"does not change with\", and covariance = \"changes with\".\\\nIdeally, we'd like features to be _invariant_ to **photometric** transformations and _covariant_ to **geometric** ones.\\\n\\\nHarris corner locations are covariant wrt translation and rotation, **but not scaling(!)**. They are also only partially invariant to affine intensity changes, due to the effects of thresholding.\n\n",
    "templates-and-histograms": "\nThese are two ways to represent image features. They can be used as **feature descriptors**.\\\n\\\n**Templates** are basically smaller images (intensities, gradients) that can be compared against the local region of a feature point.\\\n**Histograms** are simply counts or bins of the presence of certain \"sub\"-features, like particular colors or textures (e.g. oriented gradients), again in a window around the key point.\n\n",
    "scale-invariant-feature-transform-sift": "\nSIFT is an algorithm used to detect, describe, and match local features in images, invented by David Lowe in 1999. Here's [the paper](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf).\n\n",
    "sift-descriptors": "\nThese are the feature descriptors used in SIFT &mdash; each feature point is represented by a vector (of length 128) containing information about orientations in its vicinity.\\\nGradients of a 16x16 area around the point are found; then, this area is broken into 16 4x4 areas, and the gradients are binned into 8 orientations.\\\nThe algorithm also performs:\\\n\\- trilinear interpolation to smooth the output vector,\\\n\\- gaussian weighting of gradient magnitudes to prioritize orientations closest to the center of the 16x16 region,\\\n\\- some normalization/clamping of the output vector to reduce the effect of illumination,\\\n\\- dominant orientation estimation to gain invariance to rotation.\\\n\n",
    "euclidean-distance": "\nThis is a measure of the **magnitude of the difference** between two vectors.\\\nGiven two vectors `p` and `q`, it is equal to `sqrt( \u03a3( (p[i] - q[i])^2 ) )`.\n\n",
    "cosine-similarity--cosine-distance": "\nThis is a measure of the similarity between two non-zero(!) vectors in terms of the **cosine of the angle between them**.\\\nGiven two vectors `p` and `q`, it is equal to `dot(p, q) / norm(p) / norm(q)`, where `dot` produces a dot product, and `norm` gets the magnitude of the vector.\n\n",
    "nearest-neighbor-distance-ratio-nndr": "\nIn the context of feature matching, and given some notion of distance between two features, the NNDR is equal to `NN1`, the distance between a point and its nearest neighbor, divided by `NN2`, the distance between that same point and its second closest neighbor.\n\n",
    "surf-shape-context-geometric-blur-self-similarity-etc": "\nThese are all other ways to get local image description.\\\n\\\n**SURF** is a fast approximation of SIFT, which uses 2D box filters & integral images.\\\n**Shape context** bins points instead of orientations, and uses log-polar binning.\\\n... and so on. (refer to the slides if you're interested!)\n\n",
    "frequency-spectrum": "\nLight can be _completely described_ physically by its **spectrum**, the intensity of light (per unit time) for each wavelength.\\\nYou've probably seen this yourself: a rainbow, or a prism or diffraction grating, can split light into its separate wavelengths and reveal its spectrum.\n\n",
    "cones-and-rods": "\nThese are photoreceptors (light-receiving cells) that can be found in the retina of your eye.\\\n\\\n**Cones** detect light around a specific color, or to be precise, wavelength. Most humans are trichromats: we have cones corresponding to the 564\u2013580 nm, 534\u2013545 nm, and 420\u2013440 nm wavelengths, which provides our color vision.\\\n**Rods** detect light of a much wider range of wavelengths (for humans, that's the visible spectrum). They are much more sensitive than cones and thus contribute significantly to our low-light vision, but are also \"color-blind\".\n\n",
    "color-cameras": "\nCameras produce images with color by encoding three separate color channels.\\\nAt its simplest, a color camera has three sensors, one per color channel: light comes in, gets split based on its color, and hits the appropriate sensor. The requisite splitting can be achieved by an arrangement of prisms.\n\n",
    "bayer-filter": "\nBayer filters are a cheaper and more compact solution to color imaging &mdash; instead of using optical splitting and three sensors, a grid of **color filters** is placed over a square grid of sensors. Twice as many \"green\" sensors are used than red and blue, both to approximate human spectral sensitivity, and make it a _lot_ easier to tile 3 colors into a 2x2 pattern unit.\n\n",
    "cameras": "\nA camera can be understood as a **dimension reduction machine**, which maps a 3D world to a 2D image.\n\n",
    "parametric-global-transformations": "\nA parametric global transformation is one which is the same for all points p (global), and can be described by a few numbers (parameters).\n\n",
    "classes-of-transformations": "\n\\- **Displacement**: preserves distances, oriented angles, and handedness. _Translations_ only.\\\n\\- **_Proper_-Rigid**: preserves distances, **non-oriented** angles, and handedness. _Rotations_, plus the above.\\\n\\- **Rigid/Euclidean**: preserves distances and non-oriented angles. _Reflections_, plus the above.\\\n\\- **Similarity**: preserves **ratios** of distances, and non-oriented angles. _Scaling_, plus the above.\\\n\\- **Affine**: preserves ratios of distances, and the **parallel-ness** of lines. _Skewing/shearing_, plus the above.\\\n\\- **Projective**: preserves the **straightness** of lines (collinearity). _Projective warps_, plus the above.\n\n",
    "linear-transformations": "\nThese are transformations which can be represented as **matrices**, such that applying the transformation on a point is just a matter of matrix multiplication.\\\nNote that **translation** for a point in an n-dimensional space is **_not linear_**, unless you look at the transformation from an (n+1)-dimensional space (see: homogenous coordinates).\n\n",
    "homogenous-coordinates": "Converting to homogeneous coordinates requires adding a dimension. Converting from homogenous coordinates requires dividing the original dimension values by the extra dimension value that has been added when conversion. Scale invariance between homogeneous coordinates and cartesian coordinates.\n\n",
    "pinhole-cameras": "Involves sensor, pinhole or also known as camera center. Camera center is at the optical center of the camera where all light rays converge, which is the origin of camera coordinates. Focal length is the distance between the sensor and the camera center. Virtual image is projected at one focal length from the camera center towards the real object.\n\n",
    "aperture": "Aperture refers to the opening of a lens's diaphragm through which light passes. It expands and shrinks to allow more or less light through to a camera's sensor. It is a pinhole in camera. Varies in size in most cameras.\n\n",
    "lenses": "The lens captures the image and delivers it to the image sensor in the camera. Lens will vary in optical quality and price, the lens used determines the quality and resolution of the captured image. Depending on the lense you use, there may be radial distortion of the image.\n\n",
    "depth-of-field": "Depth of field is one of the essential concepts in photography. Depth of field in a photo refers to the distance between the closest and farthest objects that appears acceptably sharp. Depth of field differs based on camera type, aperture, and focusing distance. The depth of field increases with a narrower aperture and thus increases focus on an object.\n\n",
    "field-of-view-zoom": "The field of view is the extent of the observable world that is seen at any given moment. The field of view depends of focal length\nwhere a smaller field of view = larger focal length. Thus, to zoom in, you would have a larger field of view and a smaller focal length.\n\n",
    "chromatic-aberration": "Chromatic aberration, also known as color fringing, is a color distortion that creates an outline of unwanted color along the edges of objects in a photograph. Often, it appears along metallic surfaces or where there's a high contrast between light and dark objects, such as a black wall in front of a bright blue sky.\n\n",
    "barrel-and-pin-cushion-distortion": "Both are a type of fadial distortion where straight lines curve around the image center. Barrel refers to a negative distortion while pin-cushion refers to positive distortion\n\n",
    "camera-projection-matrix": "A matrix which describes the mapping of a pinhole camera from 3D points in the world to 2D points in an image.\n\n",
    "extrinsic-matrix": "Defines everything outside of the camera. Defines the coordinate space transformation between the two coordinate systems.\n\n",
    "intrinsic-matrix": "Defines everything internal of the camera, e.g. focal length, center of projection image\n\n",
    "orthographic-projection": "Orthographic projection (Also called \u201cparallel projection\u201d) where the distance from the COP to the image plane is infinite. It is a kind of parallel projection where the projecting lines emerge parallelly from the object surface and incident perpendicularly at the projecting plane.\n\n",
    "estimating-a-camera-matrix-with-known-points": "\n",
    "linear-least-squares-regression": "The line that makes the vertical distance from the data points to the regression line as small as possible. It's called a \u201cleast squares\u201d because the best line of fit is one that minimizes the variance (the sum of squares of the errors).\n\n",
    "total-least-squares-regression": "\n\n",
    "disparity": "\n",
    "triangulation-aka-estimating-depth-with-calibrated-stereo-cameras--point-correspondences": "\n",
    "finding-good-point-correspondences": "\n",
    "epipolar-constraints": "\n",
    "baseline": "\n",
    "epipoles": "\n",
    "epipolar-plane": "\n",
    "epipolar-lines": "\n",
    "essential-matrix": "\n",
    "fundamental-matrix": "\n",
    "estimating-a-fundamental-matrix-with-stereo-point-correspondences": "\n",
    "8-point-algorithm": "\n",
    "epipolar-relation": "\n",
    "random-sample-consensus-ransac": "\n",
    "lambertian-reflectance": "\n",
    "rectification": "\n",
    "correspondence-problem": "\n",
    "dense-correspondence-search": "\n",
    "similarity-cost": "\n",
    "sum-of-squared-differences-ssd": "\n",
    "normalized-correlation": "\n",
    "disparity-vs-depth": "\n",
    "uniqueness-constraint": "\n",
    "occlusion": "\n",
    "disparity-gradient-constraint": "\n",
    "ordering-constraint": "\n",
    "active-stereo-with-structured-light": "\n",
    "lidar": "\n",
    "iterative-closest-points-icp-algorithm": "\n",
    "unsupervised-learning": "\n",
    "imagenet": "\n",
    "dimension-reduction": "\n",
    "principal-component-analysis": "\n",
    "eigenfaces": "\n",
    "clustering": "\n",
    "segmentation": "\n",
    "k-means-clustering": "\n",
    "generative-discriminative": "\n",
    "agglomerative-clustering": "\n",
    "mean-shift-clustering": "\n",
    "spectral-clustering": "\n",
    "supervised-learning": "\n",
    "training-validation-and-testing-sets": "\n",
    "coverage-concision-directness": "\n",
    "classification": "\n",
    "the-machine-learning-framework": "\n",
    "nearest-neighbor-classifier": "\n",
    "voronoi-cellspartitioning": "\n",
    "k-nearest-neighbor-classifier": "\n",
    "linear-classifier": "\n",
    "na\u00efve-bayes": "\n",
    "logistic-regression": "\n",
    "support-vector-machines": "\n",
    "linear-svms": "\n",
    "nonlinear-svms": "\n",
    "kernel-trick": "\n",
    "one-vs-others-one-vs-one": "\n",
    "classification-algorithm-ideals": "\n",
    "generalization": "\n",
    "generalization-error": "\n",
    "biasvariance-trade-off": "\n",
    "underfitting": "\n",
    "overfitting": "\n",
    "generative-and-discriminative-classifiers": "\n",
    "visual-words": "\n",
    "global-image-descriptors": "\n",
    "global-texture-descriptors": "\n",
    "bag-of-words-may-be-covered-in-an-earlier-lecture": "\n",
    "gist-descriptors": "\n",
    "gaussian-mixture-model": "\n",
    "the-fisher-vector": "\n",
    "object-model": "\n",
    "dalal-triggs-pedestrian-detector": "",
    "viola-jones-face-detector": "\n",
    "boosting": "\n",
    "attention-cascade": "\n",
    "discriminative-part-based-models": "\n",
    "pascal-voc-2009": "\n",
    "paired-dictionary": "\n",
    "amazon-mechanical-turk": "",
    "separability-in-linear-spaces": "\n",
    "perceptron": "\n",
    "binary-classifier": "\n",
    "multiclass-classifier": "\n",
    "non-linearities": "\n",
    "rectified-linear-unit-relu": "\n",
    "hyperparameter": "\n",
    "convolutional-layer": "\n",
    "stride": "\n",
    "pooling-layer": "\n",
    "local-contrast-normalization": "\n",
    "hidden-unit": "\n",
    "feature-map": "\n",
    "gradient-descent": "\n",
    "loss": "\n",
    "softmax": "\n",
    "training": "\n",
    "backward-propogation": "\n",
    "stochastic-gradient-descent": "\n",
    "data-augmetation": "\n",
    "momentum": "\n",
    "regularization": "\n",
    "adversarial-patches": "\n",
    "saliency-maps": "\n",
    "local-interpretable-model-agostic-explanations-lime": "\n",
    "superpixels": "\n",
    "neural-style": "\n",
    "batch-normalization": "\nRescales each training data batch by subtracting the mean and dividing by the standard deviation per batch so that the gradients are well-behaved, in that they do not explode or vanish.\n\n",
    "alexnet": "\nA convolutional neural network architecture developed in 2012 with 8 learnable layers that achieved a 15.3% error on the ImageNet dataset.\n\n",
    "google-lenet": "\nA convolutional neural network architecture developed in 2014 with 22 learnable layers that uses inception modules containing parallel layers. LeNet improved on the results of AlexNet's performance to achieve an error of 6.67% on the ImageNet dataset.\n\n",
    "resnet": "\n",
    "decoder-networks": "\n",
    "unet": "\n",
    "two-stream-networks": ""
}